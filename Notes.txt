Dependencies installed -
pip install langchain langchain_community langchain_huggingface faiss-cpu pypdf
pip install langchain_google_genai (for gemini api)
pip install streamlit

Step 1 - setup memory for LLM (vector database)
  > load the documents
    > DirectoryLoader is used from langchain_community.document_loaders to create a loader
    > first argument is the path to search, second is document type, third is the class of the loader
    > PyPDFLoader is the loader class used from langchain_community.document_loaders
    > loader.load() loads raw text + metadata from your source file into memory.

  > create chunks
    > RecursiveCharacterTextSplitter is the class from langchain.text_splitter used to split the text
    > chunk size is how many letters will each chunk be
    > It can be between 200 - 700 dependenging on the usecase, figure it out by trying different sizes and repetative quaries
    > Chunk overlap is the amount of text, measured in characters or tokens, that is intentionally repeated between consecutive chunks
    > It is done to maintain the context between the chunks
    > Chunk overlap should be 10% - 20% of the chunk size

  > create vector embeddings
    > HuggingFaceEmbeddings is used from langchain_huggingface for creating an embedding
    > "sentence-transformers/all-MiniLM-L6-v2" is the model that is used for embedding
    > This is a sentence-transformers model: It maps sentences & paragraphs to a 384 dimensional dense vector space.
    > This model can be used for tasks like clustering or semantic search.
    > Semantic search is an advanced search technique that goes beyond keyword matching to understand the user's intent and the contextual meaning of their query
    >  By using technologies like natural language processing (NLP), machine learning (ML), and vector embeddings, it interprets the relationships between words, phrases, and concepts to deliver more accurate, relevant, and intelligent results based on semantic similarity rather than just word-for-word matches.
    > Semantic similarity is a concept in natural language processing that measures how alike two pieces of text or terms are based on their underlying meaning, or semantic content, rather than just their word usage
    * To use sentence-transformers library do [ pip install -U sentence-transformers ]
    * sentence-transformers library doesnt support keras 3. it looks for tf-keras, a shim package that makes Keras 2 work with TensorFlow. so to make it run do [ pip install tf-keras ]

  > store the embeddings in FAISS vdb
    > from langchain_community.vectorstores import FAISS
    > FAISS will store the embeddings locally unline other vector databases like pinecone, astradb that store in cloud
    > FAISS.from_documents(documents, embedding) - 1st argument is the list of documents to be added and 2nd is the embedding function to be used
    > save_local(folder_path, index_name) - 1st argument is the location where to store and 2nd is the name of the index file which is by default 'index'

Step 2 - Connect the memory from vectorstores to the LLM
  > Setting up LLM
    > langchain_huggingface allows to use LLMs through Hugging Face Inference Endpoints as if they were in langchain
    > HuggingFaceEndpoint is a bridge between LangChain and Hugging Face cloud-hosted models
    > It takes several arguments - repo_id = the repository of the model to be used, huggingfacehub_api_token = API token for authentication, temperature = ramdomness, max_new_tokens = maximum number of tokens (words/subwords) in the output response
    * In case, if you want to use an API directly without using Hugging face, just give the related parameters to the llm
  
  > Create a prompt template
    > from langchain_core.prompts import PromptTemplate is a reusable "fill-in-the-blanks" text template for prompts
    > CUSTOM_PROMPT_TEMPLATE is given a prompt to explain LLM how to answer the quaries with 2 parameters - context (data from vectorstore) and question (user input)
    > Create a PromptTemplate(template, input_variables)

  > Load the FAISS vectorstore
    > FAISS.load_local(...) simply loads an already saved FAISS vector database from disk so you can use it again (instead of recreating it from scratch)
    > load_local(folder_path, embeddings, allow_dangerous_deserialization) folder_path is the path where vectorstore is present, embeddings is the type of embeddings to be used which is HuggingFaceEmbeddings and sentence-transformers/all-MiniLM-L6-v2 model, allow_dangerous_deserialization is to define whether to allow deserialization of the data which involves loading a pickle file

  > Creating QA chain
    > from langchain.chains import RetrievalQA is used to retrieve the data from the vectorstore
    > RetrievalQA.from_chain_type() takes multiple parameters to create a chain
    > llm = llm to be used; chain_type = 'stuff' puts retrieved docs + user question together in one prompt; as_retriever = returns a retriever from vectorstore - it takes search_type -searching algorithm, search_kwargs - returns the defined number of top relevant chunks; chain_type_kwargs = It lets you pass extra arguments to customize the underlying chain (like LLMChain), it takes the prompt which has a template and input_variables

  > Invoke the chain
    > User gives an input and is sent as a query in qa_chain.invoke({'query': user_query})
    > The result is stored in response variable which has a parameter 'result' which is printed as the result from LLM

Step 3 - Creating a UI for the bot
  > creating a chat interface
    > st.chat_input({user_prompt}) is the function that creates a chat input bar at the bottom of the screen
    > st.chat_message({user}).markdown({content}) creates a chat bubble of the particular 'user' and renders the 'content'
    > st.session_state.parameter = value - this gives the access to the session store. making the content to store in the session
  
  > Loading the data from LLM
    > the vectorstore is loaded locally and stored in cache with streamlit decorator - @st.cache_resource